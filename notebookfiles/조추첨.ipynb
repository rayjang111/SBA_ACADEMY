{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-3-1c33677b8769>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-1c33677b8769>\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    f = open(\"C:/Users/edu/Desktop/+ query + '크롤.txt', 'w', encoding='utf-8')\u001b[0m\n\u001b[1;37m                                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import time, random\n",
    "\n",
    "\n",
    "def get_news(n_url):\n",
    "    news_detail = []\n",
    "    print(n_url)\n",
    "    breq = requests.get(n_url)\n",
    "    bsoup = BeautifulSoup(breq.content, 'html.parser')\n",
    "\n",
    "    title = bsoup.select('h3#articleTitle')[0].text\n",
    "    news_detail.append(title)\n",
    "\n",
    "    pdate = bsoup.select('.t11')[0].get_text()[:11]\n",
    "    news_detail.append(pdate)\n",
    "\n",
    "    _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "    btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "    news_detail.append(btext.strip())\n",
    "\n",
    "    pcompany = bsoup.select('#footer address')[0].a.get_text()\n",
    "    news_detail.append(pcompany)\n",
    "\n",
    "    return news_detail\n",
    "\n",
    "\n",
    "query = \"검색키워드\"   # url 인코딩 에러는 encoding parse.quote(query)\n",
    "s_date = \"201x.xx.xx\"\n",
    "e_date = \"201x.xx.xx\"\n",
    "s_from = s_date.replace(\".\",\"\")\n",
    "e_to = e_date.replace(\".\",\"\")\n",
    "page = 1\n",
    "\n",
    "f = open(\"C:/Users/edu/Desktop/+ query + '크롤.txt', 'w', encoding='utf-8')\n",
    "\n",
    "while page < 1000:\n",
    "\n",
    "    print(page)\n",
    "\n",
    "    url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=1&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "    #header 추가\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    }\n",
    "    req = requests.get(url,headers=header)\n",
    "    print(url)\n",
    "    cont = req.content\n",
    "    soup = BeautifulSoup(cont, 'html.parser')\n",
    "        #print(soup)\n",
    "\n",
    "    for urls in soup.select(\"._sp_each_url\"):\n",
    "        try :\n",
    "            #print(urls[\"href\"])\n",
    "            if urls[\"href\"].startswith(\"http://news.naver.com\"):\n",
    "                #print(urls[\"href\"])\n",
    "                news_detail = get_news(urls[\"href\"])\n",
    "                    # pdate, pcompany, title, btext\n",
    "                f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(news_detail[1], news_detail[3], news_detail[0],\n",
    "                                                      news_detail[2]))  # new style\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    page += 10\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser \n",
    "url = 'www.naver.com' \n",
    "webbrowser.open(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_search_url = \"http://search.naver.com/search.naver?query=\" \n",
    "search_word = '파이썬' \n",
    "url = naver_search_url + search_word \n",
    "\n",
    "webbrowser.open_new(url) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser \n",
    "google_url = \"www.google.com/#q=\" \n",
    "search_words = ['python web scraping', 'python webbrowser'] \n",
    "for search_word in search_words: \n",
    "    webbrowser.open_new(google_url + search_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_url='YouTube'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   \n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\" \n",
    "\n",
    "html_website_ranking = requests.get(url).text \n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\") \n",
    "\n",
    "# p 태그의 요소 안에서 a 태그의 요소를 찾음 \n",
    "website_ranking = soup_website_ranking.select('p a') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tmall.com\">Tmall.com</a>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0:6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this explanation'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0].get_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for title in range(6):\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={'a1':['홍재승','이화평','조상모'],\n",
    "'a2':['이병욱','장현석','강대한'],\n",
    "'a3':['명성은','배준영','유정근'],\n",
    "'a4':['신동운','박주훈','고예은'],\n",
    "'a5':['김민수','나두원'],\n",
    "'a6':['이정준','이진영','박상준'],\n",
    "'a7':['이채은','진세진','김태형'],\n",
    "'a8':['김영록','정복진'],\n",
    "'a9':['정근영','홍성규'],\n",
    "'a10':['한소희','이필주']}\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a1': [],\n",
       " 'a2': [],\n",
       " 'a3': [],\n",
       " 'a4': [],\n",
       " 'a5': [],\n",
       " 'a6': [],\n",
       " 'a7': [],\n",
       " 'a8': [],\n",
       " 'a9': [],\n",
       " 'a10': []}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a1', ['홍재승', '이화평', '조상모'])\n",
      "('a2', ['이병욱', '장현석', '강대한'])\n",
      "('a3', ['명성은', '배준영', '유정근'])\n",
      "('a4', ['신동운', '박주훈', '고예은'])\n",
      "('a5', ['김민수', '나두원'])\n",
      "('a6', ['이정준', '이진영', '박상준'])\n",
      "('a7', ['이채은', '진세진', '김태형'])\n",
      "('a8', ['김영록', '정복진'])\n",
      "('a9', ['정근영', '홍성규'])\n",
      "('a10', ['한소희', '이필주'])\n"
     ]
    }
   ],
   "source": [
    "for i in dic.items():\n",
    "    print (i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "for i in dic.items():\n",
    "    if i[1]!=[]:\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아메리카노 팀 만들기 (같은그룹 x  랜덤 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def americano():\n",
    "    dic={'a1':['홍재승','이화평','조상모'],\n",
    "'a2':['이병욱','장현석','강대한'],\n",
    "'a3':['명성은','배준영','유정근'],\n",
    "'a4':['신동운','박주훈','고예은'],\n",
    "'a5':['김민수','나두원'],\n",
    "'a6':['이정준','이진영','박상준'],\n",
    "'a7':['이채은','진세진','김태형'],\n",
    "'a8':['김영록','정복진'],\n",
    "'a9':['정근영','홍성규'],\n",
    "'a10':['한소희','이필주']}\n",
    "    copy_dic=dic\n",
    "    team_copy_dic={}\n",
    "    one=list(copy_dic.keys())[int(random.random()*10)]\n",
    "    random.shuffle(copy_dic[one])\n",
    "    team_copy_dic['1']=[]\n",
    "    team_copy_dic['1'].append(copy_dic[one].pop())\n",
    "    previous_group=[one]\n",
    "    while True:\n",
    "            group=list(copy_dic.keys())[int(random.random()*10)]\n",
    "            if group not in previous_group:\n",
    "                if len(copy_dic[group])!=0:\n",
    "                    random.shuffle(copy_dic[group])\n",
    "                    team_copy_dic[str(1)].append(copy_dic[group].pop())\n",
    "                    previous_group.append(group)\n",
    "                if len(team_copy_dic[str(1)])==6:\n",
    "                    break\n",
    "    for i in range(2,6):\n",
    "        team_copy_dic[str(i)]=[]\n",
    "        previous_group=[]\n",
    "        count=0\n",
    "        for j in dic.items():\n",
    "            if j[1]!=[]:\n",
    "                count+=1\n",
    "        if count<5:\n",
    "            return False\n",
    "        while True:\n",
    "            group=list(copy_dic.keys())[int(random.random()*10)]\n",
    "            \n",
    "            if group not in previous_group:\n",
    "                if len(copy_dic[group])!=0:\n",
    "                    random.shuffle(copy_dic[group])\n",
    "                    team_copy_dic[str(i)].append(copy_dic[group].pop())\n",
    "                    previous_group.append(group)\n",
    "                if len(team_copy_dic[str(i)])==5:\n",
    "                    break\n",
    "    return team_copy_dic\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_americano(americano):\n",
    "    while True:\n",
    "        a=americano()\n",
    "        if a!=False:\n",
    "            return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': ['장현석', '이필주', '박상준', '고예은', '정근영', '조상모'],\n",
       " '2': ['이진영', '나두원', '이채은', '김영록', '한소희'],\n",
       " '3': ['정복진', '이정준', '이병욱', '명성은', '김민수'],\n",
       " '4': ['진세진', '이화평', '홍성규', '박주훈', '배준영'],\n",
       " '5': ['강대한', '신동운', '유정근', '홍재승', '김태형']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_americano(americano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
